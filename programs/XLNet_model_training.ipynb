{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ0RD_kWvLYc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlxtend --upgrade --no-deps"
      ],
      "metadata": {
        "id": "GMB-Ybgjo9vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend                                                        \n",
        "print(mlxtend.__version__) "
      ],
      "metadata": {
        "id": "gbPpcJ0ao-Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "blHdfo855JdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "tV4SK1OW5f29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import re\n",
        "import io\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.evaluate import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "from transformers import TFXLNetModel, XLNetTokenizer\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "OE2gkDeR5i5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "JK0n6skf5qGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "5rUlkZ_m5tMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################## read data ################################\n",
        "train_data = np.load('/content/drive/MyDrive/data/sem_eval_2018/train_data.npy')\n",
        "train_labels = np.load('/content/drive/MyDrive/data/sem_eval_2018/train_labels.npy')\n",
        "\n",
        "val_data = np.load('/content/drive/MyDrive/data/sem_eval_2018/val_data.npy')\n",
        "val_labels = np.load('/content/drive/MyDrive/data/sem_eval_2018/val_labels.npy')\n",
        "\n",
        "test_data = np.load('/content/drive/MyDrive/data/sem_eval_2018/test_data.npy')\n",
        "test_labels = np.load('/content/drive/MyDrive/data/sem_eval_2018/test_labels.npy')\n"
      ],
      "metadata": {
        "id": "2khq_7sP5vz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--------------Data Shapes-----------\")\n",
        "print(\"Train data: \", train_data.shape)\n",
        "print(\"Train labels: \", train_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Validation data: \", val_data.shape)\n",
        "print(\"Validation labels: \", val_labels.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Test data: \", test_data.shape)\n",
        "print(\"Test labels: \", test_labels.shape)"
      ],
      "metadata": {
        "id": "xn9sUIvN6Ig8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing(data):\n",
        "  Tokens = []\n",
        "  finalTokens =[]\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  for i in range(len(data)):\n",
        "    tempTokens = data[i].lower() #converting to lower case\n",
        "    tempTokens = tempTokens.translate(str.maketrans('','',\"~!@#$%^&*()_-+={}[]|\\/><'?.,-+`:;1234567890\"))\n",
        "    tempTokens = tokenizer.tokenize(tempTokens) #tokenization \n",
        "    #tempTokensStopRemoval = [word for word in tempTokens if word not in stop_words] #stopword removal \n",
        "    #Tokens.append(tempTokens) # tokens with out stopword removal \n",
        "    finalTokens.append(tempTokens) # tokens after stopword removal\n",
        "    tokenised =  finalTokens\n",
        "  \n",
        "  # De-tokenized sentances\n",
        "  deTokenized = []\n",
        "  for j in range(len(finalTokens)):\n",
        "    tempTokens = []\n",
        "    tempDetoken = finalTokens[j]\n",
        "    tempDetoken = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tempDetoken]).strip()\n",
        "    deTokenized.append(tempDetoken)\n",
        "\n",
        "  return deTokenized\n"
      ],
      "metadata": {
        "id": "_UOERzPZ7leu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Data pre-processing #######################\n",
        "train_data_final = pre_processing(train_data)\n",
        "val_data_final = pre_processing(val_data)\n",
        "test_data_final = pre_processing(test_data)"
      ],
      "metadata": {
        "id": "FKnvlx679bDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Label pre-processing #######################\n",
        "train_labels_final = to_categorical(train_labels,num_classes=4)\n",
        "val_labels_final = to_categorical(val_labels,num_classes=4)\n",
        "test_labels_final = to_categorical(test_labels,num_classes=4)\n",
        "\n",
        "print(\"train label shape: \", train_labels_final.shape)\n",
        "print(\"val label shape: \", val_labels_final.shape)\n",
        "print(\"test label shape: \", test_labels_final.shape)"
      ],
      "metadata": {
        "id": "Yr2wKKvPPOji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Dataset Statistics Extractor #######################\n",
        "### Sent Conter\n",
        "def sent_counter(data):\n",
        "    sent = np.zeros((0,1))\n",
        "    for i in range(len(data)):\n",
        "        doc = data[i]\n",
        "        number_of_sentences = sent_tokenize(doc)\n",
        "        sent = np.append(sent,len(number_of_sentences))\n",
        "        \n",
        "    avg_sent = np.sum(sent)/len(data)\n",
        "    return avg_sent\n",
        "\n",
        "\n",
        "### word Conter (avg words and unique words)\n",
        "def word_count(data):\n",
        "    Tokens = []\n",
        "    totalLength = 0\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    for i in range(len(data)):\n",
        "        tempTokens = data[i].lower() #converting to lower case\n",
        "        tempTokens = tempTokens.translate(str.maketrans('','',\"~!@#$%^&*()_-+={}[]|\\/><'?.,-+`:;\"))\n",
        "        tempTokens = tokenizer.tokenize(tempTokens)\n",
        "        Tokens.append(tempTokens)\n",
        "        totalLength = totalLength + len(Tokens[i])\n",
        "    AvgWordperDocument = totalLength/len(data)\n",
        "    \n",
        "    #Unique number of words \n",
        "    totalWordlist = []\n",
        "    for j in range(len(Tokens)):\n",
        "        totalWordlist.extend(Tokens[j])   \n",
        "    wrd_counter = Counter(totalWordlist)\n",
        "        \n",
        "    return AvgWordperDocument,len(wrd_counter)\n"
      ],
      "metadata": {
        "id": "UFYIGI_K8fW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Counter\n",
        "train_sent = sent_counter(train_data_final)\n",
        "print(\"Train data: \", train_sent) \n",
        "val_sent = sent_counter(val_data_final)\n",
        "print(\"Validation data: \", val_sent) \n",
        "test_sent = sent_counter(test_data_final)\n",
        "print(\"Test data: \", test_sent)"
      ],
      "metadata": {
        "id": "Uzo407L6_Na4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Counter \n",
        "train_avg_word, train_uniqu_words = word_count(train_data_final)\n",
        "print(\"Avg word in train data: \", train_avg_word) \n",
        "print(\"Unique words in train data: \", train_uniqu_words) \n",
        "print(\"\\n\")\n",
        "\n",
        "val_avg_word, val_uniqu_words = word_count(val_data_final)\n",
        "print(\"Avg word in validation data: \", val_avg_word) \n",
        "print(\"Unique words in validation data: \", val_uniqu_words)\n",
        "print(\"\\n\") \n",
        "\n",
        "test_avg_word, test_unique_words = word_count(test_data_final)\n",
        "print(\"Avg word in test data: \", test_avg_word) \n",
        "print(\"Unique words in test data: \", test_unique_words) "
      ],
      "metadata": {
        "id": "c2u4vtvUABPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs(tweets, tokenizer, max_len=20):\n",
        "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
        "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n",
        "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
        "    ids = np.array([a['attention_mask'] for a in inps])\n",
        "    segments = np.array([a['token_type_ids'] for a in inps])\n",
        "    return inp_tok, ids, segments"
      ],
      "metadata": {
        "id": "aI2k6sUz_v7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the identifier of the model. The library need this ID to download the weights and initialize the architecture\n",
        "# here is all the supported ones:\n",
        "# https://huggingface.co/transformers/pretrained_models.html\n",
        "xlnet_model = 'xlnet-large-cased' #xlnet-base-cased-spiece.model, xlnet-large-cased\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)"
      ],
      "metadata": {
        "id": "YMQNduBDBOgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tokens, x_train_ids, x_train_segments = get_inputs(train_data_final, xlnet_tokenizer)\n",
        "x_val_tokens, x_val_ids, x_val_segments = get_inputs(val_data_final, xlnet_tokenizer)\n",
        "x_test_tokens, x_test_ids, x_test_segments = get_inputs(test_data_final, xlnet_tokenizer)"
      ],
      "metadata": {
        "id": "WylDJOs8BR2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train token shape:\" +str(x_train_tokens.shape))\n",
        "print(\"val token shape:\" +str(x_val_tokens.shape))\n",
        "print(\"test token shape:\" +str(x_test_tokens.shape))"
      ],
      "metadata": {
        "id": "l1YxiwJYBfW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tokens[1])\n",
        "print(x_val_tokens[0])\n",
        "print(x_test_tokens[0])"
      ],
      "metadata": {
        "id": "FG4P2e_YBo3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xlnet(mname):\n",
        "    \"\"\" Creates the model. It is composed of the XLNet main block and then\n",
        "    a classification head its added\n",
        "    \"\"\"\n",
        "    # Define token ids as inputs\n",
        "    word_inputs = tf.keras.Input(shape=(20,), name='word_inputs', dtype='int32')\n",
        "\n",
        "    # Call XLNet model\n",
        "    xlnet = TFXLNetModel.from_pretrained(mname)\n",
        "    xlnet_encodings = xlnet(word_inputs)[0]\n",
        "\n",
        "    # CLASSIFICATION HEAD \n",
        "    # Collect last step from last hidden state (CLS)\n",
        "    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
        "    # Apply dropout for regularization\n",
        "    #doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
        "    # Final output \n",
        "    outputs = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(doc_encoding)\n",
        "\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    #tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    # tf.keras.optimizers.Adam(lr=0.00001)\n",
        "    #model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.000005), loss=CategoricalCrossentropy(from_logits = True), metrics=CategoricalAccuracy('balanced_accuracy'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IvLK7jIgDBme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlnet = create_xlnet(xlnet_model)"
      ],
      "metadata": {
        "id": "6NEcgjUaEfOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlnet.summary()"
      ],
      "metadata": {
        "id": "XlU2qZ97Ejl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = [tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/model/xlnet.h5', verbose=1, save_best_only=True, save_weights_only=True,monitor='val_accuracy', mode='max'),\n",
        "                #tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.02, restore_best_weights=True),\n",
        "                #tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0)\n",
        "                #monitor='val_accuracy', mode='max',\n",
        "                ]"
      ],
      "metadata": {
        "id": "AH1tM-VQEpFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = xlnet.fit(x=x_train_tokens, y=train_labels_final, validation_data=(x_val_tokens,val_labels_final), epochs=25, batch_size=64, shuffle=True, callbacks=[checkpointer]) \n",
        "\n"
      ],
      "metadata": {
        "id": "EXa-iPgJUU6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dR25V928VKv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = create_xlnet(xlnet_model)\n",
        "best_model.load_weights('/content/drive/MyDrive/model/xlnet_lr00001_epoch20_noDropout_bsize64_softmax_monitorAcc.h5')"
      ],
      "metadata": {
        "id": "1qRhoWFwZ_Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test = best_model.predict(x_test_tokens)\n",
        "y_predicted = np.argmax(predict_test, axis = 1)"
      ],
      "metadata": {
        "id": "538fCh1xaMqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_names=['anger', 'fear', 'joy', 'sadness']\n",
        "print(classification_report(np.int32(test_labels), y_predicted,target_names=class_names))"
      ],
      "metadata": {
        "id": "6bsmBniFbAgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_target=np.int32(test_labels), y_predicted=np.reshape(y_predicted,(len(y_predicted),1)), binary=False)\n",
        "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
        "                                show_normed=True,\n",
        "                                cmap=\"YlGnBu\",\n",
        "                                colorbar=True,\n",
        "                                class_names=class_names)\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names,rotation=0)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.xaxis.set_ticks_position('top')"
      ],
      "metadata": {
        "id": "mddevQe0l8Kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}